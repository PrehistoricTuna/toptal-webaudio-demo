<!DOCTYPE html>
<html>
    <head>
        <title>Create browser-based audio applications controlled by MIDI hardware</title>
        <link href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.1/css/bootstrap.min.css" rel="stylesheet" />
    </head>
    <body>
        <div class="container">
            <h1>Create browser-based audio applications controlled by MIDI hardware</h1>
            <section class="intro">
                <p>
                    While the Web Audio API is increasinlgy gaining in popularity, especially amongst HTML5 game developers, the Web MIDI API is still little known amongst front-end developers. A big part of it probably has to do with its current lack of support; the Web MIDI API is currently only supported in Google Chrome, granted that you will enable a special flag for it. Browser manufacturers currently put little emphasis on this api, as it is planned to be part of ES7.<br/><br/>
                    Invented in the early 80's by several audio manufacturers, the objective was to create a simple, standard communication protocol for electronic music devices. Thirty years later, eventhough over the years other protocols such as OSC were developed, it is still the de-facto communication protocol for audio hardware manufacturers, and you will be hard-pressed to find a modern music producer that do not own at least one MIDI device in his studio.<br/><br/>
                    With the fast development and adoption of the Web Audio API, we can now start building browser-based applications that bridge the gap between the cloud and the physical world. Not only does the Web MIDI API allows us to build synthesizers and audio effects, but we can even start building browser-based DAW's (Digital Audio Workstation) similar in feature and performance to their current flash-based counterparts (Check out Audiotool, for example).<br/><br/>
                    In this tutorial, I will guide you through the basics of the Web MIDI API, and we will build a simple monosynth that you will be able to play with your favorite MIDI device. The full source code is available <a href="https://github.com/stephanepericat/toptal-webaudio-demo" target="_blank">here</a>, and you can test the <a href="http://webmididemo.herokuapp.com/" target="_blank">live demo</a> directly.
                </p>
                <h3>Pre-requisites</h3>
                <p>You will need the following to follow this tutorial:</p>
                <ul>
                    <li>The latest version of Google Chrome with the <kbd>#enable-web-midi</kbd> flag enabled</li>
                    <li>A MIDI keyboard connected to your computer</li>
                </ul>
                <p>Besides, we will be using Angular.js to bring a bit of structure to our applicaytion; therefore, basic knowledge of the framework is a pre-requisite.</p>
            </section>
            <section class="getting-started">
                <h2>Getting started</h2>
                <p>We will moduralize our application from the ground up by separating it in 3 modules:</p>
                <ul>
                    <li><b>WebMIDI:</b> handling the various MIDI devices connected to your computer</li>
                    <li><b>WebAudio:</b> providing the audio source for our synth</li>
                    <li><b>WebSynth:</b> connecting the web interface to the audio engine</li>
                </ul>
                <p>Finally, an App module will handle the user interaction with the web interface. Our application structure could look a bit like this:</p>
                <pre>
        |-app
        |--js
        |--- midi.js
        |--- audio.js
        |--- synth.js
        |--- app.js
        |- index.html
                </pre>
                <p>You should also install the following libraries to help you build up your aplication: <kbd>Angular.js</kbd>, <kbd>Boostrap</kbd> and <kbd>jQuery</kbd>. Probably the easiest way to do so is via Bower.</p>
            </section>
            <section class="WebMidi-module">
                <h2>The WebMidi module: connecting with the real world</h2>
                <p>Let's start by connecting our MIDI devices to our app. To do so, we will create a simple factory returning a single method. To connect to our MIDI devices via the Web MIDI API, we need to call the <kbd>navigator.requestMIDIAccess()</kbd> method:</p>
                <pre>
angular
    .module('WebMIDI', [])
    .factory('Devices', ['$window', function($window) {
        function _connect() {
            if($window.navigator && 'function' === typeof $window.navigator.requestMIDIAccess) {
                $window.navigator.requestMIDIAccess();
            } else {
                throw 'No Web MIDI support';
            }
        }

        return {
            connect: _connect
        };
    }]);
                </pre>
                <p>And that's pretty much it!<br/><br/>The requestMIDIAccess method returns a promise, so we can just return it directly and we will be handling the result of the promise in our app's controller:</p>
                <pre>
angular
    .module('DemoApp', ['WebMIDI'])
    .controller('AppCtrl', ['$scope', 'Devices', function($scope, devices) {
        $scope.devices = [];

        devices
            .connect()
            .then(function(access) {
                if('function' === typeof access.inputs) {
                    // deprecated
                    $scope.devices = access.inputs();
                    console.error('Update your Chrome version!');
                } else {
                    if(access.inputs && access.inputs.size > 0) {
                        var inputs = access.inputs.values(),
                            input = null;

                        // iterate through the devices
                        for (input = inputs.next(); input && !input.done; input = inputs.next()) {
                            $scope.devices.push(input.value);
                        }
                    } else {
                        console.error('No devices detected!');
                    }

                }
            })
            .catch(function(e) {
                console.error(e);
            });
    }]);
                </pre>
                <p>As mentionned, the requestMIDIAccess method returns a promise, passing an <i>access</i> Object to the then() method, with two properties: <kbd>inputs</kbd> and <kbd>outputs</kbd>.<br/><br/>In earlier versions of Chrome, these two properties were methods allowing you to retrieve an array of input and output devices directly. In the latest updates however, these properties are now objects. This makes quite a difference, since we now need to call the values() method on either the inputs or outputs object to retrieve the corresponding list of devices. This method acts as a generator function, and returns an iterator. Again, this API is meant to be part of ES7; therefore, implementing a generator-like behavior made a lot of sense, eventhough it is not as straight-forward as the original implementation.<br/>Finally, we can retrieve the amount of devices via the size property of the iterator object, and if there is at least one device, we simply iterate over the result by calling the next() method of the iterator object, and pushing each device to an array defined on the $scope. On the front-end, we can implement a simple select box which will list all the available input devices and let us choose which device we want to use as active device to control the web synth:</p>
                <pre>
&lt;select ng-model="activeDevice" class="form-control" ng-options="device.manufacturer + ' ' + device.name for device in devices"&gt;
    &lt;option value="" disabled>Choose a MIDI device...&lt;/option&gt;
&lt;/select&gt;
                </pre>
                <p>We bound this select box to a $scope variable called <kbd>activeDevice</kbd>. We will use this later to connect this active device to the synth.</p>
            </section>
            <section class="WebAudio-module">
                <h2>The WebAudio module: making noise</h2>
                <p>The WebAudio API allows us to not only play sound files, but also generate sounds by recreating the essential components of synthesizers such as oscillators, filters and gain nodes <a href="https://developer.mozilla.org/en-US/docs/Web/API/AudioContext#Methods" target="_blank">amongst others</a>.</p>
                <h3>Create an oscillator</h3>
                <p>The role of oscillators is to output a wave form. There are various types of wave forms, amongst which four are supported in the WebAudio API: <kbd>sine</kbd>, <kbd>square</kbd>, <kbd>triangle</kbd> and <kbd>sawtooth</kbd>. Wave forms are said to "oscillate" at a certain frequency. A certain range of frequencies are audible by human beings; they are known as sounds. Alternatively, when they are oscillating at low frequencies, oscillators can also help us build LFO's ("low frequency oscillator") so we can modulate our sounds (but this is out of the scope of this tutorial).<br/><br/>The first thing we need to do to create some sounds is to instantiate a new AudioContext:</p>
                <pre>
function _createContext() {
    self.ctx = new $window.AudioContext();
}
                </pre>
                <p>From there, we can instantiate any of the components made available by the WebAudio API. Since we might create multiple instances of each components, it makes sense to create services to be able to create new, unique instances of the components we need. Let's start by creating the service to generate a new oscillator:</p>
                <pre>
angular
    .module('WebAudio', [])
    .service('OSC', function() {
        var self;

        function Oscillator(ctx) {
            self = this;
            self.osc = ctx.createOscillator();

            return self;
        }
    });
                </pre>
                <p>We can now instantiate new oscillators at our will, passing as unique argument the AudioContext instance we created earlier. Now, we can build some wrapper methods for syntaxic sugar, and return the Oscillator function:</p>
                <pre>
Oscillator.prototype.setOscType = function(type) {
    if(type) {
        self.osc.type = type
    }
}

Oscillator.prototype.setFrequency = function(freq, time) {
    self.osc.frequency.setTargetAtTime(freq, 0, time);
};

Oscillator.prototype.start = function(pos) {
    self.osc.start(pos);
}

Oscillator.prototype.stop = function(pos) {
    self.osc.stop(pos);
}

Oscillator.prototype.connect = function(i) {
    self.osc.connect(i);
}

Oscillator.prototype.cancel = function() {
    self.osc.frequency.cancelScheduledValues(0);
}

return Oscillator;
                </pre>
                <h3>Create a multipass filter and a volume control</h3>
                <p>We need two more components to complete our basic audio engine: a multipass filter, to give a bit of shape to our sound, and a gain node to control the volume of our sound and turn the volume on and off. To do so, we can proceed in the same way we did for the oscillator: create services returning a function with some wrapper methods. All we need to do is provide the AudioContext instance and call the appropriate method.</p>
                <p>To create a filter:</p>
                <pre>
ctx.createBiquadFilter();
                </pre>
                <p>To create a gain node:</p>
                <pre>
ctx.createGain();
                </pre>
            </section>
            <section class="WebSynth-module">
                <h2>The WebSynth module: wiring things up</h2>
                <p>Now we are almost ready to build our synth interface and connect our midi devices to our audio source. FIrst, we need to connect our audio engine together and get it ready to receive midi notes. To connect the audio engine, we simply create new instances of the components that we need, and then "connect" them together using the connect() method available for each components' instances. The connect() method takes one argument, which is simply the component you want to connect the current instance to.</p>
                <pre>
self.osc1 = new Oscillator(self.ctx);
self.osc1.setOscType('sine');
self.amp = new Amp(self.ctx);

self.osc1.connect(self.amp.gain);

self.amp.connect(self.ctx.destination);
self.amp.setVolume(0.0, 0); //mute the sound

self.osc1.start(0); // start osc1
                </pre>
                <p>We will also need some logic to enable and disable the filter. The filter will be connected between the volume and the destination:</p>
                <pre>
self.filter1 = new Filter(self.ctx);
self.filter1.setFilterFrequency(50);
self.filter1.setFilterResonance(0);

function _connectFilter() {
    self.amp.disconnect();
    self.amp.connect(self.filter1.filter);
    self.filter1.connect(self.ctx.destination);
}

function _disconnectFilter() {
    self.filter1.disconnect();
    self.amp.disconnect();
    self.amp.connect(self.ctx.destination);
}
                </pre>
                <p>We just built the internal wiring of our audio engine. You can play around a bit and try different combinations of wiring, but remember to turn down the volume to not become deaf| Now we can hook up the MIDI interface to our application and send MIDI messages to the audio engine. We will setup a watcher on the device selectbox to virtually "plug" it to our synth. We will then listen to MIDI messages incoming from the device, and pass the information to the audio engine:</p>
                <pre>
// in the app's controller
$scope.$watch('activeDevice', DSP.plug);

// in the synth module
function _onmidimessage(e) {
    /**
    * e.data is an array
    * e.data[0] = on (144) / off (128) / detune (224)
    * e.data[1] = midi note
    * e.data[2] = velocity || detune
    */
    switch(e.data[0]) {
        case 144:
        Engine.noteOn(e.data[1], e.data[2]);
        break;
        case 128:
        Engine.noteOff(e.data[1]);
        break;
    }

}

function _plug(device) {
    self.device = device;
    self.device.onmidimessage = _onmidimessage;
}
                </pre>
                <p>Here, we are listening to MIDI events from the device, analysing the data from the <a href="http://www.w3.org/TR/2012/WD-webmidi-20121213/#midievent-interface" target="_blank">MidiEvent Object</a>, and passing it to the appropriate method, either noteOn() or noteOff(), based on the event code (144 for noteOn, 128 for noteOff). We can now add the logic in the respective methods in the audio module to actually generate a sound:</p>
                <pre>
function _noteOn(note, velocity) {
    self.activeNotes.push(note);

    self.osc1.cancel();
    self.currentFreq = _mtof(note);
    self.osc1.setFrequency(self.currentFreq, self.settings.portamento);

    self.amp.cancel();

    self.amp.setVolume(1.0, self.settings.attack);
}

function _noteOff(note) {
    var position = self.activeNotes.indexOf(note);
    if (position !== -1) {
        self.activeNotes.splice(position, 1);
    }

    if (self.activeNotes.length === 0) {
        // shut off the envelope
        self.amp.cancel();
        self.currentFreq = null;
        self.amp.setVolume(0.0, self.settings.release);
    } else {
        // in case another note is pressed, we set that one as the new active note
        self.osc1.cancel();
        self.currentFreq = _mtof(self.activeNotes[self.activeNotes.length - 1]);
        self.osc1.setFrequency(self.currentFreq, self.settings.portamento);
    }
}
                </pre>
                <p>A few things are happening here. In the noteOn() method, we first push the current note to an array of notes. Eventhough we are building a monosynth (meaning we can only play one note at a time), we can still have several fingers at once on the keyboard so we need to queue all theses notes so that when we release one note, the next one is played. We then need to stop the oscillator to assign the new frequrency, which we convert from a MIDI note (scale from 0 to 127) to actual frequency value with a bit of math:</p>
                <pre>
function _mtof(note) {
    return 440 * Math.pow(2, (note - 69) / 12);
}
                </pre>
                <p>In the noteOff() method, we first start by finding the note in the array of active notes and removing it. Then if, it was the only note in the array, we simply turn off the volume.<br/>The second argument of the setVolume() method is the transition time, meaning how long it takes the gain to reach the new volume value. In musical terms, if the note is on, it would be the equivalent of the attack time, and if the note is off, it is the equivalent of the release time.</p>
            </section>
            <section class="WebAnalyser-module">
                <h2>The WebAnalyser module: visualising our sound</h2>
                <p>Another interesting feature we can add to our synth is an analyser node, allowing us to display the wave form of our sound using canvas to render it. Creating an analyser node is a bit more complicated than other AudioContext objects, as it requires to also create a scriptProcessor node to actually perform the analysis. We start by creating selecting the canvas element on the DOM:</p>
                <pre>
function Analyser(canvas) {
    self = this;

    self.canvas = angular.element(canvas) || null;
    self.view = self.canvas[0].getContext('2d') || null;
    self.javascriptNode = null;
    self.analyser = null;

    return self;
}
                </pre>
                <p>Then, we add a connect() method, in which we will create both the analyser and the script processor:</p>
                <pre>
Analyser.prototype.connect = function(ctx, output) {
    // setup a javascript node
    self.javascriptNode = ctx.createScriptProcessor(2048, 1, 1);
    // connect to destination, else it isn't called
    self.javascriptNode.connect(ctx.destination);

    // setup an analyzer
    self.analyser = ctx.createAnalyser();
    self.analyser.smoothingTimeConstant = 0.3;
    self.analyser.fftSize = 512;

    // connect the output to the destiantion for sound
    output.connect(ctx.destination);
    // connect the output to the analyser for processing
    output.connect(self.analyser);

    self.analyser.connect(self.javascriptNode);

    // define the colors for the graph
    var gradient = self.view.createLinearGradient(0, 0, 0, 200);
    gradient.addColorStop(1, '#000000');
    gradient.addColorStop(0.75, '#ff0000');
    gradient.addColorStop(0.25, '#ffff00');
    gradient.addColorStop(0, '#ffffff');

    // when the audio process event is fired on the script processor
    // we get the frequency data into an array
    // and pass it to the drawSpectrum method to render it in the canvas
    self.javascriptNode.onaudioprocess = function() {
        // get the average for the first channel
        var array =  new Uint8Array(self.analyser.frequencyBinCount);
        self.analyser.getByteFrequencyData(array);

        // clear the current state
        self.view.clearRect(0, 0, 1000, 325);

        // set the fill style
        self.view.fillStyle = gradient;
        drawSpectrum(array);
    }
};
                </pre>
                <p>We first create a scriptProcessor object and connect it to the destination. Then we create the analyser itself, which we feed with the audio output from the oscillator or filter. Notice how we still need to connect the audio output to the destination so we can hear it! We also need to define the gradient colors of our graph; this is done by calling the createLinearGradient() method of the canvas element.<br/><br/>Finally, the scriptProcessor will fire an 'audioprocess' event on an interval; when this event is fired, we calculate the average frequencies captured by the analyser, we clear the canvas and redraw the new frequency graph by calling the drawSpectrum() method:</p>
                <pre>
function drawSpectrum(array) {
    for (var i = 0; i < (array.length); i++) {
        var v = array[i],
        h = self.canvas.height();

        self.view.fillRect(i * 2, h - (v - (h / 4)), 1, v + (h / 4));
    }
}
                </pre>
                <p>Last but not least, we will need to modify the wiring of our audio engine a bit to accomodate this new component:</p>
                <pre>
// in the _connectFilter() method
if(self.analyser) {
    self.analyser.connect(self.ctx, self.filter1);
} else {
    self.filter1.connect(self.ctx.destination);
}

// in the _disconnectFilter() method
if(self.analyser) {
    self.analyser.connect(self.ctx, self.amp);
} else {
    self.amp.connect(self.ctx.destination);
}
                </pre>
                <p>We now have a nice visualiser which allow us to display the waveform of our synth in real time! This is a bit of work to setup, but it's very interesting and insightful, especially when using filters.</p>
            </section>
            <section class="building-upon">
                <h2>Building up on our synth: adding velocity &amp; detune</h2>
                <p>we now have a pretty cool synth. But it plays every note at the same volume. This is because instead of handling the velocity data properly, we simply set the volume to a fix value of <kbd>1.0</kbd>. Let's start by fixing that, and then we will see how we can enable the detune wheel that you can find on most common MIDI keyboards.</p>
                <h3>Enabling velocity</h3>
                <p>If you are unfamiliar with it, the 'velocity' relates to how hard you hit the key on your keyboard. Based on this value, the sound created seems either softer or louder.<br/><br/>In our synth, we can emulate this behavior by simply playing with the volume of the gain node. To do so, we first need to do a bit of math to convert the MIDI data into a float value between 0.0 and 1.0 to pass to the gain node:</p>
                <pre>
function _vtov (velocity) {
    return (velocity / 127).toFixed(2);
}
                </pre>
                <p>The velocity range of a MIDI device is from 0 to 127, so we simply divide that value by 127 and return a float value with two decimals. Then, we can update the _noteOn() method to pass the calcluated value to the gain node:</p>
                <pre>
self.amp.setVolume(_vtov(velocity), self.settings.attack);
                </pre>
                <p>And that's it! now when we play our synth, we will notice the volumes varies based on how hard we hit the keys on our keyboard.</p>
                <h3>Enabling the detune wheel on your MIDI keyboard</h3>
                <p>Most MIDI keyboards feature a detune wheel; this allows you to slightly alter the frequency of the note currently being played, creating an interesting effect known as 'detune'. This is fairly easy to implement, since the detune wheel also fires a MidiMessage event with its own event code (224), which we can listen to and act upon by recalculating the frequency value and update the oscillator.</p>
                <p>We first need to catch the event in our synth. To do so, we add an extra case on the switch we creatd in the _onmidimessage() callback:</p>
                <pre>
case 224:
    // the detune value is the third argument of the MidiEvent.data array
    Engine.detune(e.data[2]);
break;
                </pre>
                <p>Then, we define the detune() method on the audio engine:</p>
                <pre>
function _detune(d) {
    if(self.currentFreq) {
        //64 = no detune
        if(64 === d) {
            self.osc1.setFrequency(self.currentFreq, self.settings.portamento);
            self.detuneAmount = 0;
        } else {
            var detuneFreq = Math.pow(2, 1 / 12) * (d - 64);
            self.osc1.setFrequency(self.currentFreq + detuneFreq, self.settings.portamento);
            self.detuneAmount = detuneFreq;
        }
    }
}
                </pre>
                <p>The default detune value is 64, which means there is not detune applied, so in this case we simply pass the current frequency to the oscillator.</p>
                </p>Finally, we also need to update the _noteOff() method, to take the detune in consideration in case another note is queued:</p>
                <pre>
self.osc1.setFrequency(self.currentFreq + self.detuneAmount, self.settings.portamento);
                </pre>
            </section>
        </div>
    </body>
</html>
